---
layout: post
title: "3D Light Field Video & the End of Green Screens"
tags: video
---

If you’ve never worked in video post-production then you’ve never known the frustration of trying to “key” or isolate something from a green screen. It’s what makes the weather anchor look like they’re standing in front of the weather map. It is an ingrained and fundamental skill in the video post-production workflow. In its ideal form (as pitched by the software makers of course) the process is always simple. In reality small variations in lighting, mistakes made by the talent, or poor camera work mean the editor spends far too much time replacing that background.

The heavy lifting for this process has been in post while the production side has been focused on other improvements. The speed of upheaval on the digital camera side has been blistering in recent years - <a title="Canon EOS 5D" href="http://en.wikipedia.org/wiki/Canon_EOS_5D" rel="wikipedia" target="_blank">Canon 5D</a> & 7D, <a title="Red Cameras" href="http://www.red.com" rel="homepage" target="_blank">RED</a>, <a title="Arri Alexa" href="http://www.arridigital.com/" rel="homepage" target="_blank">Arri Alexa</a>, <a href="http://www.blackmagic-design.com/products/blackmagiccinemacamera/">Blackmagic Cinema Camera</a> - but nothing will alter the editing workflow like <a title="Light field" href="http://en.wikipedia.org/wiki/Light_field" rel="wikipedia" target="_blank">light field</a> technology.

The primary trick that light field companies like <a title="Lytro" href="http://www.lytro.com/" rel="homepage" target="_blank">Lytro</a> are promoting is the ability to change focus after a picture is taken. The technology that allows this to take place is impressive, because the camera is capturing all of the light and the direction of the light rays in the scene at the moment you take the picture.This is made possible by a tiny array of microlenses mounted just in front of the image sensor that allow the software to analyze slight differences in light and calculate the various positions of objects in the scene.

Now move forward to <a href="http://raytrix.de/">Raytrix</a>, a German company that sells high-end 3D light field video cameras and you can start to imagine where this goes. Raytrix cameras can generate a 3D model of a subject in real-time. And by real-time, I mean instantly! The product demos alone are worth the <a href="http://www.gputechconf.com/gtcnew/on-demand-gtc.php?sessionTopic=&searchByKeyword=raytrix&submit=&select=+&sessionEvent=&sessionYear=&sessionFormat#1513">39 minute talk</a> in my opinion.

All footage from the Raytrix camera includes a depth map indicating positions of objects in the scene. Now imagine instead of altering the focal plane you could use the depth map to create a “keying” plane allowing the editor to remove objects not based on their color (green or blue) but based on their distance from the camera. All of sudden actors could be separated from the background and placed into keyed environments all based on the 3D map created in real-time while filming. This approach of using the depth plane for keying would be far superior to the current green screen process.

This light field technology is not just a future product announcement. The <a href="http://raytrix.de/index.php/Cameras.html">Raytrix R5 camera</a> shoots at 30fps and currently retails for 3,990 Euro. The real hurdle now is imagining and creating the tools that can use this new depth data. Post-production software will need time to catch up to this new leap forward in camera technology. I can’t wait to see how quickly this will bring the demise of green screens.
